{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_ml_36.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VigneshwaraChinnadurai/ML-Learning/blob/master/Session_36_Natural_Language_Processing/NLP_ml_36.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yah2azqq5D6t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "07835690-32e8-498d-f208-36ddee900858"
      },
      "source": [
        "#STEP-1: Install Import Libraries\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "#STEP-2: Autheticate E-Mail ID\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#STEP-3: Get File from Drive using file-ID\n",
        "\n",
        "#2.1 Get the file\n",
        "downloaded = drive.CreateFile({'id':'1ITLd-8hRZfn6EJYhJ_LEHgQWuinxjCOB'})\n",
        "# replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('Restaurant_Reviews.tsv') \n",
        "\n",
        "#STEP-4: Read File\n",
        "\n",
        "#3.1 Read file as panda dataframe\n",
        "import pandas as pd\n",
        "data = pd.read_csv('Restaurant_Reviews.tsv',delimiter='\\t',quoting=3) \n",
        "print(data.head())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 20.0MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 3.3MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 4.8MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 3.7MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 4.4MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 5.7MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 4.4MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 4.9MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 4.9MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 4.9MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133kB 4.9MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 163kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 174kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 194kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 204kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 225kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 235kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 256kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 266kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 286kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 296kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 317kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 327kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 348kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 358kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 378kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 389kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 409kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 419kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 440kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 450kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 471kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 481kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 501kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 512kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 532kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 542kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 563kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 573kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 593kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 604kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 624kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 634kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 655kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 665kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 686kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 696kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 716kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 727kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 747kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 757kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 768kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 778kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 788kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 798kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 808kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 819kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 829kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 839kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 849kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 860kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 870kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 880kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 890kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 901kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 911kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 921kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 931kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 942kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 952kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 962kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 972kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 983kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 993kB 4.9MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "                                              Review  Liked\n",
            "0                           Wow... Loved this place.      1\n",
            "1                                 Crust is not good.      0\n",
            "2          Not tasty and the texture was just nasty.      0\n",
            "3  Stopped by during the late May bank holiday of...      1\n",
            "4  The selection on the menu was great and so wer...      1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhyMNe1G5u_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp_6ilw75vbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=pd.read_csv('Restaurant_Reviews.tsv', delimiter='\\t',quoting=3)\n",
        "# Delimiteer to mention that this is tsv and quoting is to ignore double quotes in the dataset."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDIqTw1q51Ne",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ae954245-d0f3-4b6e-f814-9b05a090e33f"
      },
      "source": [
        "# Cleaning the texts\n",
        "import re\n",
        "import nltk\n",
        "# NLTK library has all list of words which is irrelavent to the review. Hence downloading the set of words.\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "# This PORTER_STEMMER is to make a single word irrespective of its tense (Present,past,future tense)\n",
        "corpus = []\n",
        "# CORPUS (Meaning) collection of texts in NLP\n",
        "for i in range(0, 1000):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', data['Review'][i])\n",
        "    # This will replace anything apart from alphabets with space in the given dataset.\n",
        "    review = review.lower()\n",
        "    # To change anything in uppercase to lowercase.\n",
        "    review = review.split()\n",
        "    ps = PorterStemmer()\n",
        "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    # Adding the word to review list when the for loop is successfully completed.\n",
        "    # IF NOT. When the condition fails, it adds the word to the list.\n",
        "    # SET function is used to make the algorithm faster. (Mandatory in case we read a book.)\n",
        "    review = ' '.join(review)\n",
        "    # This is to join back the list of words into sentence. space dot join is to mention the delimiter is space.\n",
        "    corpus.append(review)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJZSH3tI54dn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the Bag of Words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features = 1500)\n",
        "# MAX_FEATUREis to filter non relevant words.This value is selected after creating X and checcking the column size.\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "# Here X is sparce matrix \n",
        "# Cleaning of data can be done in COUNT_VECTORIZER Class which has parameters such as stopwords,token_parameter,lowercase,etc.\n",
        "y = data.iloc[:, 1].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om-K-JQT58EN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
        "\n",
        "# Fitting Naive Bayes to the Training set\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jMSv7wI5_DV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f8f93969-4867-4c7c-d310-039856910024"
      },
      "source": [
        "cm"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[55, 42],\n",
              "       [12, 91]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JAz5Ns-6Ans",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f6749ee-a40a-4233-b4d9-d20cddc42dc0"
      },
      "source": [
        "print('Accuracy is ',(91+55)/200)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is  0.73\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}